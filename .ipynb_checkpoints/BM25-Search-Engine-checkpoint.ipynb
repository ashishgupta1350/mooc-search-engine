{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line.toml',\n",
       " 'moocs-queries.txt',\n",
       " '.ipynb_checkpoints',\n",
       " 'moocs-qrels.txt',\n",
       " 'moocs.dat',\n",
       " 'metadata.dat',\n",
       " 'BM25-Search-Engine.ipynb']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_file = \"./moocs-queries.txt\"\n",
    "qrels_file = './moocs-qrels.txt'\n",
    "moocs_data = './moocs.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23275 1\n",
      "\n",
      "0 12146 1\n",
      "\n",
      "0 8229 1\n",
      "\n",
      "0 7194 1\n",
      "\n",
      "0 16153 1\n",
      "\n",
      "0 23179 1\n",
      "\n",
      "0 5943 1\n",
      "\n",
      "0 130 1\n",
      "\n",
      "0 12388 1\n",
      "\n",
      "0 13538 1\n",
      "\n",
      "0 2146 1\n",
      "\n",
      "0 132 1\n",
      "\n",
      "0 67 1\n",
      "\n",
      "0 16211 1\n",
      "\n",
      "0 6238 1\n",
      "\n",
      "0 10 1\n",
      "\n",
      "0 22241 1\n",
      "\n",
      "0 8030 1\n",
      "\n",
      "0 12929 1\n",
      "\n",
      "1 10850 1\n",
      "\n",
      "1 11890 1\n",
      "\n",
      "1 8260 1\n",
      "\n",
      "2 11890 1\n",
      "\n",
      "2 1646 1\n",
      "\n",
      "2 11892 1\n",
      "\n",
      "2 23386 1\n",
      "\n",
      "2 5892 1\n",
      "\n",
      "3 132 1\n",
      "\n",
      "3 15087 1\n",
      "\n",
      "3 298 1\n",
      "\n",
      "3 9607 1\n",
      "\n",
      "3 23501 1\n",
      "\n",
      "3 2492 1\n",
      "\n",
      "3 7194 1\n",
      "\n",
      "3 16211 1\n",
      "\n",
      "3 19932 1\n",
      "\n",
      "3 130 1\n",
      "\n",
      "3 12388 1\n",
      "\n",
      "3 12929 1\n",
      "\n",
      "4 5006 1\n",
      "\n",
      "4 6808 1\n",
      "\n",
      "4 15820 1\n",
      "\n",
      "4 15573 1\n",
      "\n",
      "4 15552 1\n",
      "\n",
      "4 21124 1\n",
      "\n",
      "4 15763 1\n",
      "\n",
      "4 15790 1\n",
      "\n",
      "4 15556 1\n",
      "\n",
      "4 175 1\n",
      "\n",
      "4 3010 1\n",
      "\n",
      "4 383 1\n",
      "\n",
      "4 15636 1\n",
      "\n",
      "4 18340 1\n",
      "\n",
      "4 15812 1\n",
      "\n",
      "4 15754 1\n",
      "\n",
      "4 15576 1\n",
      "\n",
      "4 15713 1\n",
      "\n",
      "5 23422 1\n",
      "\n",
      "5 19550 1\n",
      "\n",
      "5 15164 1\n",
      "\n",
      "5 19680 1\n",
      "\n",
      "5 11845 1\n",
      "\n",
      "5 19332 1\n",
      "\n",
      "5 5806 1\n",
      "\n",
      "5 2609 1\n",
      "\n",
      "5 7505 1\n",
      "\n",
      "5 12164 1\n",
      "\n",
      "6 15010 1\n",
      "\n",
      "6 23275 1\n",
      "\n",
      "6 12146 1\n",
      "\n",
      "6 16153 1\n",
      "\n",
      "6 7194 1\n",
      "\n",
      "6 12 1\n",
      "\n",
      "6 23179 1\n",
      "\n",
      "6 130 1\n",
      "\n",
      "6 12388 1\n",
      "\n",
      "6 13075 1\n",
      "\n",
      "6 132 1\n",
      "\n",
      "6 67 1\n",
      "\n",
      "6 22794 1\n",
      "\n",
      "6 10 1\n",
      "\n",
      "6 12929 1\n",
      "\n",
      "6 15099 1\n",
      "\n",
      "7 18040 1\n",
      "\n",
      "7 788 1\n",
      "\n",
      "7 3489 1\n",
      "\n",
      "7 12061 1\n",
      "\n",
      "7 20070 1\n",
      "\n",
      "7 17215 1\n",
      "\n",
      "7 18059 1\n",
      "\n",
      "7 5719 1\n",
      "\n",
      "7 3662 1\n",
      "\n",
      "7 4857 1\n",
      "\n",
      "7 3176 1\n",
      "\n",
      "7 8988 1\n",
      "\n",
      "7 12048 1\n",
      "\n",
      "7 12240 1\n",
      "\n",
      "8 4881 1\n",
      "\n",
      "8 2992 1\n",
      "\n",
      "8 1401 1\n",
      "\n",
      "8 5369 1\n",
      "\n",
      "8 2978 1\n",
      "\n",
      "8 11104 1\n",
      "\n",
      "8 19204 1\n",
      "\n",
      "8 816 1\n",
      "\n",
      "8 15172 1\n",
      "\n",
      "8 3582 1\n",
      "\n",
      "9 11078 1\n",
      "\n",
      "9 17267 1\n",
      "\n",
      "9 11108 1\n",
      "\n",
      "9 21524 1\n",
      "\n",
      "9 22897 1\n",
      "\n",
      "9 8989 1\n",
      "\n",
      "10 16001 1\n",
      "\n",
      "10 23272 1\n",
      "\n",
      "10 13270 1\n",
      "\n",
      "10 1983 1\n",
      "\n",
      "10 94 1\n",
      "\n",
      "10 2483 1\n",
      "\n",
      "10 4207 1\n",
      "\n",
      "10 1999 1\n",
      "\n",
      "10 23 1\n",
      "\n",
      "11 13073 1\n",
      "\n",
      "11 184 1\n",
      "\n",
      "11 14279 1\n",
      "\n",
      "11 18559 1\n",
      "\n",
      "11 23562 1\n",
      "\n",
      "11 12 1\n",
      "\n",
      "12 591 1\n",
      "\n",
      "12 14433 1\n",
      "\n",
      "12 7103 1\n",
      "\n",
      "12 13400 1\n",
      "\n",
      "12 13786 1\n",
      "\n",
      "12 19986 1\n",
      "\n",
      "12 16505 1\n",
      "\n",
      "Intro to Computer Science & Programming Course - Udacity https://www.udacity.com/course/cs101  Sign In Sign Up Catalog Nanodegree Intro to Computer Science Build a Search Engine & a Social Network    Beginner Approx. 3 months  Assumes 6hr/wk (work at your own pace)  Join 113,366 Students  VIEW TRAILER Course Summary  In this introduction to computer programming course, you’ll learn and practice key computer science concepts by building your own versions of popular web applications. You’ll learn Python, a powerful, easy-to-learn, and widely used programming language, and you’ll explore computer science basics, as you build your own search engine and social network.  Why Take This Course?  You’ll learn the programming language Python, and you’ll explore foundational concepts in computer science. Most importantly, you’ll start thinking like a software engineer by solving interesting problems (how to build a web crawler or a social network) using computer programming.  This course is a first step into the world of computer science, and whether you want to become a software engineer, or collaborate with software engineers, this course is for you. You’ll be prepared for intermediate-level computer science classes when you’ve mastered the concepts covered in this course.  Build a Search Engine:  Throughout this course, you’ll build a search engine by learning about and producing key search engine components including a crawler, an index and a page rank algorithm. As you build these pieces, you’ll be learning about and practicing computer science skills that will ready you for intermediate level computer science courses.  Build a Social Network:  At the end of the course we will give you a set of relationships (i.e. strings of phrases like “Dave likes Andy, Kathleen and Kristy”) and you will use your new computer science skills to organize these relationships into a social network. With your new social network, you can explore relationships and gain insight into how you fit into your own social networks.  Prerequisites and Requirements  There is no prior computer programming knowledge needed for this course. Beginners are welcome!  See the Technology Requirements for using Udacity.  What Will I Learn?  Projects  Build a search engine throughout this course. Then, build a social network with your new skills!  Syllabus  Lesson 1: How to Get Started  Interview with Sergey Brin Getting Started with Python Processors Grace Hopper Variables Strings and Numbers Indexing Strings String Theory Lesson 2: How to Repeat  Introducing Procedures Sum Procedure with a Return Statement Equality Comparisons If Statements Or Function Biggest Procedure While Loops Print Numbers Lesson 2.5: How to Solve Problems  What are the Inputs Algorithm Pseudocode Optimizing Lesson 3: How to Manage Data  Nested Lists A List of Strings Aliasing List Operations List Addition and Length How Computers Store Data For Loops Popping Elements Crawl Web Lesson 4: Responding to Queries  Data Structures Lookup Building the Web Index Latency Bandwidth Buckets of Bits Protocols Lesson 5: How Programs Run  Measuring Speed Spin Loop Index Size vs. Time Making Lookup Faster Hash Function Testing Hash Functions Implementing Hash Tables Dictionaries Modifying the Search Engine Lesson 6: How to Have Infinite Power  Infinite Power Counter Recursive Definitions Recursive Procedures Palindromes Recursive v. Iterative Divide and Be Conquered Ranking Web Pages Lesson 7: Past, Present, and the Future of Computing  Past of Computing Computer History Museum First Hard Drive Search Before Computers Present of Computing Slac and Big Data Open Source Future of Computing Text Analysis Energy Aware Computing Computer Security Quantum Computing Enroll in Course  Start free trial $199/month after 14-day trial Best for learners serious about course completion & career advancement  What you get Instructor videos See All  Access Course Materials  Access course materials Free  What you get Instructor videos Learn by doing exercises and view project instructions Projects with reviews Stuck? Get help from Coaches Verified Certificate Instructors & Partners   ▾ Dave Evans INSTRUCTOR David Evans is a Professor of Computer Science at the University of Virginia where he teaches computer science and leads research in computer security. He is the author of an introductory computer science textbook and has won Virginia's highest award for university faculty. He has PhD, SM, and SB degrees from MIT.   View more courses in Data Science   View more courses in Web Development   View more courses in Software Engineering  Frequently Asked Questions  When does the course begin?  This class is self paced. You can begin whenever you like and then follow your own pace. It’s a good idea to set goals for yourself to make sure you stick with the course.  How long will the course be available?  This class will always be available!  How do I know if this course is for me?  Take a look at the “Class Summary,” “What Should I Know,” and “What Will I Learn” sections above. If you want to know more, just enroll in the course and start exploring.  Can I skip individual videos? What about entire lessons?  Yes! The point is for you to learn what YOU need (or want) to learn. If you already know something, feel free to skip ahead. If you ever find that you’re confused, you can always go back and watch something that you skipped.  What are the rules on collaboration?  Collaboration is a great way to learn. You should do it! The key is to use collaboration as a way to enhance learning, not as a way of sharing answers without understanding them.  Why are there so many questions?  Udacity classes are a little different from traditional courses. We intersperse our video segments with interactive questions. There are many reasons for including these questions: to get you thinking, to check your understanding, for fun, etc... But really, they are there to help you learn. They are NOT there to evaluate your intelligence, so try not to let them stress you out.  What should I do while I’m watching the videos?  Learn actively! You will retain more of what you learn if you take notes, draw diagrams, make notecards, and actively try to make sense of the material.  INFORMATION  Nanodegree Credentials  Georgia Tech Program  Udacity for Organizations  Help and FAQ  Feedback Program  COMMUNITY  Blog  News & Media  Developer API  UDACITY  About  Jobs  Contact Us  Legal  FOLLOW US ON      MOBILE APPS     Nanodegree is a trademark of Udacity © 2011-2015 Udacity, Inc.  \n",
      "\n",
      "Intro to Physics Fundamentals Course (Physics 101) - Udacity https://www.udacity.com/course/ph100   Sign In Sign Up Catalog Nanodegree Intro to Physics Landmarks in Physics    Beginner Approx. 2 months  Assumes 6hr/wk (work at your own pace)  Join 17,946 Students  VIEW TRAILER Course Summary  Study physics abroad in Europe -- virtually! Learn the basics of physics on location in Italy, the Netherlands and the UK, by answering some of the discipline's major questions from over the last 2000 years.  Why Take This Course?  This unique class gives you the chance to see the sites where physics history was made and learn some of the subject's most captivating concepts.  Prerequisites and Requirements  This course is suitable for anyone; a basic understanding of algebra is suggested.  See the Technology Requirements for using Udacity.  What Will I Learn?  Syllabus  Lesson 1: How can we measure the circumference of the Earth?  Basics of geometry and trigonometry  Lesson 2: How do objects move?  Data analysis and kinematics  Lesson 3: What causes motion?  Forces, acceleration, and Newton’s Laws  Lesson 4: How can we use motion?  Work, energy, and simple machines  Lesson 5: How can we determine our longitude at sea?  Simple harmonic motion  Lesson 6: What is electricity?  Charge and electric fields  Lesson 7: What is left to discover?  Modern physics and open questions  Start Free Course  Start free course Free  What you get Instructor videos Learn by doing exercises Instructors & Partners   ▾ Andy Brown INSTRUCTOR Andy Brown is a Lead Instructor at Udacity. He has a degree in physics from MIT, and has devoted his time after graduating to teaching and learning. Since joining Udacity in 2012, Andy has helped design dozens of classes. He loves exploring this new educational medium by creating innovative and engaging courses. When he’s not figuring out the best ways to transform minds, you can find Andy on his bike or in his car, exploring the amazing experiences that his new California home has to offer.  Frequently Asked Questions  When does the course begin?  This class is self paced. You can begin whenever you like and then follow your own pace. It’s a good idea to set goals for yourself to make sure you stick with the course.  How long will the course be available?  This class will always be available!  How do I know if this course is for me?  Take a look at the “Class Summary,” “What Should I Know,” and “What Will I Learn” sections above. If you want to know more, just enroll in the course and start exploring.  Can I skip individual videos? What about entire lessons?  Yes! The point is for you to learn what YOU need (or want) to learn. If you already know something, feel free to skip ahead. If you ever find that you’re confused, you can always go back and watch something that you skipped.  How much does this cost?  It’s completely free! If you’re feeling generous, we would love to have you contribute your thoughts, questions, and answers to the course discussion forum.  What are the rules on collaboration?  Collaboration is a great way to learn. You should do it! The key is to use collaboration as a way to enhance learning, not as a way of sharing answers without understanding them.  Why are there so many questions?  Udacity classes are a little different from traditional courses. We intersperse our video segments with interactive questions. There are many reasons for including these questions: to get you thinking, to check your understanding, for fun, etc... But really, they are there to help you learn. They are NOT there to evaluate your intelligence, so try not to let them stress you out.  What should I do while I’m watching the videos?  Learn actively! You will retain more of what you learn if you take notes, draw diagrams, make notecards, and actively try to make sense of the material.  INFORMATION  Nanodegree Credentials  Georgia Tech Program  Udacity for Organizations  Help and FAQ  Feedback Program  COMMUNITY  Blog  News & Media  Developer API  UDACITY  About  Jobs  Contact Us  Legal  FOLLOW US ON      MOBILE APPS     Nanodegree is a trademark of Udacity © 2011-2015 Udacity, Inc.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryFile = open(queries_file)\n",
    "qrelsFile = open(qrels_file)\n",
    "moocsFile = open(moocs_data)\n",
    "# for line in queryFile.readlines():\n",
    "#     print(line)\n",
    "for line in qrelsFile.readlines():\n",
    "    print(line)\n",
    "\n",
    "for line in moocsFile.readlines()[:2]:\n",
    "    print(line)\n",
    "queryFile.close()\n",
    "qrelsFile.close()\n",
    "moocsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# meta_data = './metadata.dat'\n",
    "# metaData = open(meta_data)\n",
    "\n",
    "# moocs_data = './moocs.dat'\n",
    "# moocsFile = open(moocs_data)\n",
    "\n",
    "# for line in metaData.readlines()[:3]:\n",
    "#     print(line)\n",
    "# print(\"****************\")\n",
    "# print()\n",
    "# print()\n",
    "# for line in moocsFile.readlines()[:2]:\n",
    "#     print(line)\n",
    "    \n",
    "    \n",
    "# moocsFile.close()\n",
    "# metaData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17972, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moocsDataFrame = pd.DataFrame(columns = ['doc_id', 'doc_text'])\n",
    "moocs_data = './moocs.dat'\n",
    "moocsFile = open(moocs_data)\n",
    "allMoocStrings  = []\n",
    "docIndex=1\n",
    "for line in moocsFile.readlines():\n",
    "    moocString = [docIndex, line]\n",
    "    allMoocStrings.append(moocString)\n",
    "    docIndex+=1\n",
    "# len(allMoocStrings)    \n",
    "moocDataFrame = pd.DataFrame(allMoocStrings,columns = ['doc_id','doc_text'])\n",
    "moocDataFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Intro to Computer Science &amp; Programming Course...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Intro to Physics Fundamentals Course (Physics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Intro to Statistics - Udacity https://www.udac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Introduction to Algorithms Course Online - Uda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Web Development Online Course - Udacity https:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                           doc_text\n",
       "0       1  Intro to Computer Science & Programming Course...\n",
       "1       2  Intro to Physics Fundamentals Course (Physics ...\n",
       "2       3  Intro to Statistics - Udacity https://www.udac...\n",
       "3       4  Introduction to Algorithms Course Online - Uda...\n",
       "4       5  Web Development Online Course - Udacity https:..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moocDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'former', 'onto', 'some', 'hereupon', 'were', 'give', 'first', 'as', 'whoever', 'becomes', 'our', 'whom', 'anyone', 'ours', 'thence', 'nevertheless', 'could', 'might', 'thereby', 'system', 'top', 'its', 're', 'eg', 'sometime', 'very', 'on', 'elsewhere', 'when', 'amongst', 'seem... 'yourselves', 'etc', 'along', 'ie', 'who', 'around', 'other', 'those', 'toward', 'during', 'that'}),\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intro': 151, 'computer': 55, 'science': 273, 'programming': 246, 'course': 63, 'udacity': 329, 'https': 130, 'www': 355, 'com': 50, 'cs101': 69, 'sign': 283, 'catalog': 42, 'nanodegree': 201, 'build': 39, 'search': 274, 'engine': 90, 'social': 290, 'network': 205, 'beginner': 30, 'approx': 19, 'months': 199, 'assumes': 22, '6hr': 6, 'wk': 351, 'work': 353, 'pace': 217, 'join': 157, '113': 0, '366': 5, 'students': 309, 'view': 341, 'trailer': 326, 'summary': 311, 'introduction': 153, 'll': 181, 'learn': 167, 'practice': 232, 'key': 160, 'concepts': 58, 'building': 40, 'versions': 338, 'popular': 229, 'web': 348, 'applications': 18, 'python': 252, 'powerful': 231, 'easy': 86, 'widely': 350, 'used': 333, 'language': 164, 'explore': 101, 'basics': 28, 'foundational': 110, 'importantly': 133, 'start': 297, 'thinking': 322, 'like': 176, 'software': 291, 'engineer': 91, 'solving': 293, 'interesting': 147, 'problems': 239, 'crawler': 67, 'using': 334, 'step': 301, 'world': 354, 'want': 344, 'collaborate': 48, 'engineers': 93, 'prepared': 234, 'intermediate': 148, 'level': 175, 'classes': 46, 've': 336, 'mastered': 190, 'covered': 65, 'learning': 169, 'producing': 243, 'components': 54, 'including': 134, 'index': 135, 'page': 219, 'rank': 256, 'algorithm': 12, 'pieces': 226, 'practicing': 233, 'skills': 285, 'ready': 258, 'courses': 64, 'end': 88, 'set': 281, 'relationships': 262, 'strings': 306, 'phrases': 225, 'dave': 71, 'likes': 177, 'andy': 15, 'kathleen': 159, 'kristy': 163, 'use': 332, 'new': 207, 'organize': 216, 'gain': 117, 'insight': 141, 'fit': 108, 'networks': 206, 'prerequisites': 235, 'requirements': 264, 'prior': 238, 'knowledge': 162, 'needed': 203, 'beginners': 31, 'welcome': 349, 'technology': 317, 'projects': 249, 'syllabus': 313, 'lesson': 172, 'started': 298, 'interview': 150, 'sergey': 280, 'brin': 37, 'getting': 119, 'processors': 242, 'grace': 122, 'hopper': 129, 'variables': 335, 'numbers': 211, 'indexing': 136, 'string': 305, 'theory': 321, 'repeat': 263, 'introducing': 152, 'procedures': 241, 'sum': 310, 'procedure': 240, 'return': 268, 'statement': 299, 'equality': 97, 'comparisons': 52, 'statements': 300, 'function': 114, 'biggest': 34, 'loops': 186, 'print': 237, 'solve': 292, 'inputs': 140, 'pseudocode': 251, 'optimizing': 214, 'manage': 189, 'data': 70, 'nested': 204, 'lists': 179, 'list': 178, 'aliasing': 13, 'operations': 213, 'addition': 9, 'length': 171, 'computers': 56, 'store': 303, 'popping': 228, 'elements': 87, 'crawl': 66, 'responding': 266, 'queries': 254, 'structures': 307, 'lookup': 184, 'latency': 165, 'bandwidth': 27, 'buckets': 38, 'bits': 35, 'protocols': 250, 'programs': 247, 'run': 271, 'measuring': 193, 'speed': 295, 'spin': 296, 'loop': 185, 'size': 284, 'vs': 343, 'time': 323, 'making': 188, 'faster': 105, 'hash': 125, 'testing': 318, 'functions': 115, 'implementing': 132, 'tables': 314, 'dictionaries': 79, 'modifying': 197, 'infinite': 138, 'power': 230, 'counter': 62, 'recursive': 261, 'definitions': 74, 'palindromes': 221, 'iterative': 155, 'divide': 81, 'conquered': 60, 'ranking': 257, 'pages': 220, 'past': 223, 'present': 236, 'future': 116, 'computing': 57, 'history': 128, 'museum': 200, 'hard': 124, 'drive': 85, 'slac': 288, 'big': 33, 'open': 212, 'source': 294, 'text': 319, 'analysis': 14, 'energy': 89, 'aware': 26, 'security': 276, 'quantum': 253, 'enroll': 95, 'free': 111, 'trial': 327, '199': 2, 'month': 198, '14': 1, 'day': 73, 'best': 32, 'learners': 168, 'completion': 53, 'career': 41, 'advancement': 10, 'instructor': 143, 'videos': 340, 'access': 7, 'materials': 192, 'doing': 83, 'exercises': 100, 'project': 248, 'instructions': 142, 'reviews': 269, 'stuck': 308, 'help': 126, 'coaches': 47, 'verified': 337, 'certificate': 43, 'instructors': 144, 'partners': 222, 'evans': 99, 'david': 72, 'professor': 244, 'university': 331, 'virginia': 342, 'teaches': 315, 'leads': 166, 'research': 265, 'author': 23, 'introductory': 154, 'textbook': 320, 'won': 352, 'highest': 127, 'award': 25, 'faculty': 103, 'phd': 224, 'sm': 289, 'sb': 272, 'degrees': 75, 'mit': 195, 'development': 77, 'engineering': 92, 'frequently': 112, 'asked': 21, 'questions': 255, 'does': 82, 'begin': 29, 'class': 45, 'self': 278, 'paced': 218, 'follow': 109, 'good': 121, 'idea': 131, 'goals': 120, 'make': 187, 'sure': 312, 'stick': 302, 'long': 182, 'available': 24, 'know': 161, 'look': 183, 'sections': 275, 'just': 158, 'exploring': 102, 'skip': 286, 'individual': 137, 'entire': 96, 'lessons': 173, 'yes': 356, 'point': 227, 'need': 202, 'feel': 107, 'ahead': 11, 'confused': 59, 'watch': 345, 'skipped': 287, 'rules': 270, 'collaboration': 49, 'great': 123, 'way': 347, 'enhance': 94, 'sharing': 282, 'answers': 16, 'understanding': 330, 'little': 180, 'different': 80, 'traditional': 325, 'intersperse': 149, 'video': 339, 'segments': 277, 'interactive': 146, 'reasons': 260, 'check': 44, 'fun': 113, 'really': 259, 'evaluate': 98, 'intelligence': 145, 'try': 328, 'let': 174, 'stress': 304, 'watching': 346, 'actively': 8, 'retain': 267, 'notes': 210, 'draw': 84, 'diagrams': 78, 'notecards': 209, 'sense': 279, 'material': 191, 'information': 139, 'credentials': 68, 'georgia': 118, 'tech': 316, 'program': 245, 'organizations': 215, 'faq': 104, 'feedback': 106, 'community': 51, 'blog': 36, 'news': 208, 'media': 194, 'developer': 76, 'api': 17, 'jobs': 156, 'contact': 61, 'legal': 170, 'mobile': 196, 'apps': 20, 'trademark': 324, '2011': 3, '2015': 4}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words=my_stop_words)\n",
    "X = vectorizer.fit_transform(moocDataFrame.values[:1][:,1])\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "moocDataFrame.head(2)\n",
    "# moocDataFrame.loc[2].doc_text\n",
    "from nltk.tokenize import word_tokenize\n",
    "# word_tokenize(“this’s a test”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK dependencies\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for pos tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    \n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# list of stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "moocDataFrameLematized = moocDataFrame.head(0).copy()\n",
    "\n",
    "def tokenize_lematize(moocDataFrame):\n",
    "    # create a copy of moocDataFrame\n",
    "    moocDataFrameLematized = moocDataFrame.head(0).copy()\n",
    "    \n",
    "    for i in range(len(moocDataFrame)):\n",
    "        # convert sentences to sentence tokens\n",
    "        text = moocDataFrame.loc[i].doc_text\n",
    "        sent_tokenize_list = sent_tokenize(text)\n",
    "        # lematized_paragraph is a list, that will be joined by ' ' later to get final para\n",
    "        lematized_paragraph = []\n",
    "        for sent in sent_tokenize_list:\n",
    "            # convert each sentence to word tokens\n",
    "            sent = sent.lower()\n",
    "            word_tokenize_list = word_tokenize(sent) \n",
    "            new_sent_list = []\n",
    "            \n",
    "            for token, tag in pos_tag(word_tokenize_list):\n",
    "                # pos tagging using wordnet lematizer and lematizing words to reduce corpus\n",
    "                lemma_word = wordnet_lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "                \n",
    "                # remove stop words and string punctuations.\n",
    "                if lemma_word not in stop_words and lemma_word not in string.punctuation:\n",
    "                    new_sent_list.append(lemma_word)\n",
    "            \n",
    "            # get the sentence back\n",
    "            sent = ' '.join(new_sent_list)\n",
    "            \n",
    "            # append the lematized sentence\n",
    "            lematized_paragraph.append(sent)\n",
    "            \n",
    "        # join sentences finally to get paragraph.The datatype of paragraph changes\n",
    "        lematized_paragraph = ' '.join(lematized_paragraph)\n",
    "        d = {\n",
    "            'doc_id' : i+1,\n",
    "            'doc_text' : lematized_paragraph\n",
    "        }\n",
    "        moocDataFrameLematized = moocDataFrameLematized.append(pd.Series(d), ignore_index = True)\n",
    "    return moocDataFrameLematized\n",
    "        #         moocDataFrameLematized.loc[i].doc_text = lematized_paragraph\n",
    "            \n",
    "def tokenize_lematize_query(query='This is a dummy query.'):\n",
    "    sent_tokenize_list = sent_tokenize(query)\n",
    "    # lematized_paragraph is a list, that will be joined by ' ' later to get final para\n",
    "    lematized_query = []\n",
    "    for sent in sent_tokenize_list:\n",
    "        # convert each sentence to word tokens\n",
    "        sent = sent.lower()\n",
    "        word_tokenize_list = word_tokenize(sent) \n",
    "        new_sent_list = []\n",
    "        for token, tag in pos_tag(word_tokenize_list):\n",
    "            # pos tagging using wordnet lematizer and lematizing words to reduce corpus\n",
    "            lemma_word = wordnet_lemmatizer.lemmatize(token, tag_map[tag[0]])\n",
    "            # remove stop words and string punctuations.\n",
    "            if lemma_word not in stop_words and lemma_word not in string.punctuation:\n",
    "                new_sent_list.append(lemma_word)\n",
    "        sent = ' '.join(new_sent_list)\n",
    "        # append the lematized sentence\n",
    "        lematized_query.append(sent)\n",
    "    return ' '.join(lematized_query)\n",
    "    \n",
    "# helper\n",
    "def count(word, split_text):\n",
    "    count = 0\n",
    "    for w in split_text:\n",
    "        if w == word:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "# helper\n",
    "def indexBOW(bow_dictionary):\n",
    "    # key is word and value is frequency count; Just return index and word count\n",
    "    # again key value pairs in dictionaries\n",
    "    \n",
    "    idxtoword = {}\n",
    "    wordtoidx = {}\n",
    "    words = list(bow_dictionary.keys())\n",
    "    for i in range(len(bow_dictionary.keys())):\n",
    "        idxtoword[i] = words[i]\n",
    "        wordtoidx[words[i]] = i\n",
    "    return idxtoword, wordtoidx\n",
    "\n",
    "def bag_of_words(list_of_text):\n",
    "    # assumed ['text para 1', 'text para 2' ...]\n",
    "    # key value pair\n",
    "    bow = {} \n",
    "    for text in list_of_text:\n",
    "        split_text = text.split(\" \")\n",
    "        words = set(split_text)\n",
    "        for word in words:\n",
    "            # we want to count the word in split_text(so we dont have to split the text again and again)\n",
    "            if word in bow.keys():\n",
    "                bow[word] += count(word, split_text)\n",
    "            else:\n",
    "                bow[word] = 1 # 1 occurance defined\n",
    "                \n",
    "    idxToWord, wordToIdx = indexBOW(bow)\n",
    "    return bow, idxToWord, wordToIdx # these are dictionaries\n",
    "\n",
    "def vectorize_sentence(sentence,wordtoidx):\n",
    "    # assumed text is 'this is a text'\n",
    "    split = sentence.split()\n",
    "    # [ 0,0,...,1,2,0, ...,1,5,0,...]\n",
    "    doc_vector = []\n",
    "    allWords = list(wordtoidx.keys())\n",
    "    for i in range(len(wordtoidx.keys())):\n",
    "        doc_vector.append(count(allWords[i], split))\n",
    "    return doc_vector\n",
    "\n",
    "def vectorSpaceModel(someInputs):\n",
    "    pass\n",
    "\n",
    "def searchEngine():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dummy query'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moocDataFrameLematized = tokenize_lematize(moocDataFrame.head(5))\n",
    "# moocDataFrameLematized.doc_text.iloc[1]\n",
    "tokenize_lematize_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = “this’s a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.”\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "len(sent_tokenize_list)\n",
    "\n",
    "\n",
    "\n",
    "# PunktTokenizer splits on punctuation, but keeps it with the word:\n",
    "\n",
    "from nltk.tokenize import PunktWordTokenizer\n",
    "punkt_word_tokenizer = PunktWordTokenizer()\n",
    "punkt_word_tokenizer.tokenize(“this’s a test”)\n",
    "# [‘this’, “‘s”, ‘a’, ‘test’]\n",
    "\n",
    "# WordPunctTokenizer splits all punctuations into separate tokens:\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "word_punct_tokenizer.tokenize(“This’s a test”)\n",
    "# [‘This’, “‘”, ‘s’, ‘a’, ‘test’]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now we have to do the pos tagging of the words before lematizing them\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    " \n",
    "s = \"This is a simple sentence\"\n",
    "tokens = word_tokenize(s) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) \n",
    " \n",
    "print(tokens_pos)\n",
    "    \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize(‘dogs’)\n",
    "# u’dog’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'a', 'a', 'a', 'string', 'string', 'string']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'this is a a a a string string string'\n",
    "s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def count(word, split_text):\n",
    "    count = 0\n",
    "    for w in split_text:\n",
    "        if w == word:\n",
    "            count +=1\n",
    "    return count\n",
    "\n",
    "\n",
    "# helper\n",
    "def indexBOW(bow_dictionary):\n",
    "    # key is word and value is frequency count; Just return index and word count\n",
    "    # again key value pairs in dictionaries\n",
    "    \n",
    "    idxtoword = {}\n",
    "    wordtoidx = {}\n",
    "    words = list(bow_dictionary.keys())\n",
    "    for i in range(len(bow_dictionary.keys())):\n",
    "        idxtoword[i] = words[i]\n",
    "        wordtoidx[words[i]] = i\n",
    "    return idxtoword, wordtoidx\n",
    "\n",
    "def bag_of_words(list_of_text):\n",
    "    # assumed ['text para 1', 'text para 2' ...]\n",
    "    # key value pair\n",
    "    bow = {} \n",
    "    for text in list_of_text:\n",
    "        split_text = text.split(\" \")\n",
    "        words = set(split_text)\n",
    "        for word in words:\n",
    "            # we want to count the word in split_text(so we dont have to split the text again and again)\n",
    "            if word in bow.keys():\n",
    "                bow[word] += count(word, split_text)\n",
    "            else:\n",
    "                bow[word] = 1 # 1 occurance defined\n",
    "                \n",
    "    idxToWord, wordToIdx = indexBOW(bow)\n",
    "    return bow, idxToWord, wordToIdx # these are dictionaries\n",
    "\n",
    "def vectorize_sentence(sentence,wordtoidx):\n",
    "    # assumed text is 'this is a text'\n",
    "    split = sentence.split()\n",
    "    # [ 0,0,...,1,2,0, ...,1,5,0,...]\n",
    "    doc_vector = []\n",
    "    allWords = list(wordtoidx.keys())\n",
    "    for i in range(len(wordtoidx.keys())):\n",
    "        doc_vector.append(count(allWords[i], split))\n",
    "    return doc_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 0, 'some': 1, 'there': 2, 'is': 3, 'he': 4, 'great,': 5, 'more': 6}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = bag_of_words(['there is some text', 'he is great, there is some more text'])\n",
    "vectorize_sentence('there is is is some text', z)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {}\n",
    "a['hello'] = 1\n",
    "'hello' in a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'a', 'z', 'c']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['a'] = 1\n",
    "a['z'] = 1\n",
    "a['c'] = 1\n",
    "list(a.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hello', 'a', 'z', 'c'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxtoword = {}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
